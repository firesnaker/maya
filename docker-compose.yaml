services:
  # --- Ollama Service ---
  ollama:
    # Use the official Ollama Docker image
    image: ollama/ollama
    container_name: ollama
    # Ollama typically runs on port 11434
    ports:
      - "11434:11434"
    # This mounts a volume to persist the models you download
    # If you stop/remove the container, your models will still be there
    volumes:
      - ollama_models:/root/.ollama
    # Run a simple model immediately after the container starts
    #command: ["/bin/sh", "-c", "ollama serve & ollama pull llama3:8b"]
    command: ["/bin/sh", "-c", "ollama serve & ollama pull gemma2:2b-instruct"]
    # Adjust this model name (llama3:8b) based on your hardware constraints. 
    # Smaller models like phi3:mini are better for resource-limited machines.

  # --- Backend Service ---
  backend:
    build:
      context: ./be-go  # Directory where the backend Dockerfile is located
      dockerfile: Dockerfile
    container_name: go-backend
    ports:
      # Map container port 8080 to host port 8080
      - "8080:8080"
    environment:
      # This is how you set the API key and other variables
      # The values here will be injected into the Go service
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      # ðŸ’¡ New: Set the LLM API URL for the Go backend
      - LLM_API_URL=http://ollama:11434
    depends_on:
      # ðŸ’¡ New: Ensure Ollama is running before the backend tries to connect
      ollama:
        condition: service_started
    # Optional: If your Go app relies on a database, you would define it here
    #   - database

  # --- Frontend Service ---
  frontend:
    build:
      context: ./fe-next # Directory where the frontend Dockerfile is located
      dockerfile: Dockerfile
    container_name: next-frontend
    ports:
      # Map container port 3000 to host port 3000
      - "3000:3000"
    environment:
      # Next.js variables need to be prefixed with NEXT_PUBLIC_ if accessed on the client side
      - NEXT_PUBLIC_BACKEND_URL=http://backend:8080 
      # Note: For communication between frontend and backend *inside* Docker Compose,
      # use the service name ('backend') as the hostname, not 'localhost'.
    depends_on:
      - backend # Start the backend before the frontend
      - ollama # Ensures LLM is running before FE starts (optional, but clean)
      
# --- Define the Volume ---
volumes:
  ollama_models:
    driver: local
